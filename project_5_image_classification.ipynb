{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Artificial Intelligence\\n\",\n",
    "    \"# 464\\n\",\n",
    "    \"# Project #5 - Image Classification\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Before You Begin...\\n\",\n",
    "    \"00. We're using a Jupyter Notebook environment (tutorial available here: https://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/what_is_jupyter.html),\\n\",\n",
    "    \"01. Read the entire notebook before beginning your work, and\\n\",\n",
    "    \"02. Check the submission deadline on Gradescope.\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"## General Directions for this Assignment\\n\",\n",
    "    \"00. Output format should be exactly as requested,\\n\",\n",
    "    \"01. Functions should do only one thing.\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Before You Submit...\\n\",\n",
    "    \"00. Re-read the general instructions provided above, and\\n\",\n",
    "    \"01. Hit \\\"Kernel\\\"->\\\"Restart & Run All\\\". The first cell that is run should show [1], the second should show [2], and so on...\\n\",\n",
    "    \"02. Submit your notebook (as .ipynb, not PDF) using Gradescope, and\\n\",\n",
    "    \"03. Do not submit any other files.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Neural Networks: Image Classification\\n\",\n",
    "    \"\\n\",\n",
    "    \"For this assignment, we will explore Neural Networks for image classification using the MNIST dataset, which consists of handwritten digits (0-9). The goal is to classify these images correctly. We'll use PyTorch to explore different model architectures and compare their performance.\\n\",\n",
    "    \"\\n\",\n",
    "    \"More specifically: \\n\",\n",
    "    \"\\n\",\n",
    "    \"* Try three different models (varying in complexity),\\n\",\n",
    "    \"* Use cross validation to compare the models,\\n\",\n",
    "    \"* Report the winning model's performance on test data,\\n\",\n",
    "    \"* Include a table comparing the three models,\\n\",\n",
    "    \"* Document your process: what the results were, how the winning model was determined, loss function, activation, etc.\\n\",\n",
    "    \"\\n\",\n",
    "    \"Information on cross validation: https://scikit-learn.org/stable/modules/cross_validation.html (notice how final evaluation is the only time test data is used). No other directions for this assignment other than what's here and in the \\\"General Directions\\\" section. You have a lot of freedom with this assignment. Don't get carried away. It is expected the results may vary, being better or worse, due to the limitations of the dataset. Graders are not going to run your notebooks. The notebook will be read as a report on how different models were explored. Since you'll be using libraries, the emphasis will be on your ability to communicate your findings.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import torch\\n\",\n",
    "    \"import torch.nn as nn\\n\",\n",
    "    \"import torch.optim as optim\\n\",\n",
    "    \"from torch.utils.data import DataLoader\\n\",\n",
    "    \"from torchvision import datasets, transforms\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"from sklearn.model_selection import KFold\\n\",\n",
    "    \"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Load MNIST dataset\\n\",\n",
    "    \"transform = transforms.Compose([\\n\",\n",
    "    \"    transforms.ToTensor(),\\n\",\n",
    "    \"    transforms.Normalize((0.1307,), (0.3081,))\\n\",\n",
    "    \"])\\n\",\n",
    "    \"\\n\",\n",
    "    \"train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\\n\",\n",
    "    \"test_dataset = datasets.MNIST('./data', train=False, transform=transform)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Split training data into train and validation sets\\n\",\n",
    "    \"train_size = int(0.85 * len(train_dataset))\\n\",\n",
    "    \"val_size = len(train_dataset) - train_size\\n\",\n",
    "    \"train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create data loaders\\n\",\n",
    "    \"train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\\n\",\n",
    "    \"val_loader = DataLoader(val_dataset, batch_size=64)\\n\",\n",
    "    \"test_loader = DataLoader(test_dataset, batch_size=64)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Define three different models\\n\",\n",
    "    \"class SimpleCNN(nn.Module):\\n\",\n",
    "    \"    def __init__(self):\\n\",\n",
    "    \"        super(SimpleCNN, self).__init__()\\n\",\n",
    "    \"        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\\n\",\n",
    "    \"        self.relu = nn.ReLU()\\n\",\n",
    "    \"        self.maxpool = nn.MaxPool2d(2)\\n\",\n",
    "    \"        self.fc1 = nn.Linear(5408, 128)\\n\",\n",
    "    \"        self.fc2 = nn.Linear(128, 10)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"    def forward(self, x):\\n\",\n",
    "    \"        x = self.conv1(x)\\n\",\n",
    "    \"        x = self.relu(x)\\n\",\n",
    "    \"        x = self.maxpool(x)\\n\",\n",
    "    \"        x = x.view(x.size(0), -1)\\n\",\n",
    "    \"        x = self.fc1(x)\\n\",\n",
    "    \"        x = self.relu(x)\\n\",\n",
    "    \"        x = self.fc2(x)\\n\",\n",
    "    \"        return x\\n\",\n",
    "    \"\\n\",\n",
    "    \"class MediumCNN(nn.Module):\\n\",\n",
    "    \"    def __init__(self):\\n\",\n",
    "    \"        super(MediumCNN, self).__init__()\\n\",\n",
    "    \"        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\\n\",\n",
    "    \"        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\\n\",\n",
    "    \"        self.relu = nn.ReLU()\\n\",\n",
    "    \"        self.maxpool = nn.MaxPool2d(2)\\n\",\n",
    "    \"        self.dropout = nn.Dropout(0.25)\\n\",\n",
    "    \"        self.fc1 = nn.Linear(1600, 128)\\n\",\n",
    "    \"        self.fc2 = nn.Linear(128, 10)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"    def forward(self, x):\\n\",\n",
    "    \"        x = self.conv1(x)\\n\",\n",
    "    \"        x = self.relu(x)\\n\",\n",
    "    \"        x = self.conv2(x)\\n\",\n",
    "    \"        x = self.relu(x)\\n\",\n",
    "    \"        x = self.maxpool(x)\\n\",\n",
    "    \"        x = self.dropout(x)\\n\",\n",
    "    \"        x = x.view(x.size(0), -1)\\n\",\n",
    "    \"        x = self.fc1(x)\\n\",\n",
    "    \"        x = self.relu(x)\\n\",\n",
    "    \"        x = self.dropout(x)\\n\",\n",
    "    \"        x = self.fc2(x)\\n\",\n",
    "    \"        return x\\n\",\n",
    "    \"\\n\",\n",
    "    \"class ComplexCNN(nn.Module):\\n\",\n",
    "    \"    def __init__(self):\\n\",\n",
    "    \"        super(ComplexCNN, self).__init__()\\n\",\n",
    "    \"        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\\n\",\n",
    "    \"        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\\n\",\n",
    "    \"        self.conv3 = nn.Conv2d(64, 128, kernel_size=3)\\n\",\n",
    "    \"        self.relu = nn.ReLU()\\n\",\n",
    "    \"        self.maxpool = nn.MaxPool2d(2)\\n\",\n",
    "    \"        self.dropout = nn.Dropout(0.25)\\n\",\n",
    "    \"        self.bn1 = nn.BatchNorm2d(32)\\n\",\n",
    "    \"        self.bn2 = nn.BatchNorm2d(64)\\n\",\n",
    "    \"        self.bn3 = nn.BatchNorm2d(128)\\n\",\n",
    "    \"        self.fc1 = nn.Linear(128, 64)\\n\",\n",
    "    \"        self.fc2 = nn.Linear(64, 10)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"    def forward(self, x):\\n\",\n",
    "    \"        x = self.conv1(x)\\n\",\n",
    "    \"        x = self.bn1(x)\\n\",\n",
    "    \"        x = self.relu(x)\\n\",\n",
    "    \"        x = self.conv2(x)\\n\",\n",
    "    \"        x = self.bn2(x)\\n\",\n",
    "    \"        x = self.relu(x)\\n\",\n",
    "    \"        x = self.maxpool(x)\\n\",\n",
    "    \"        x = self.conv3(x)\\n\",\n",
    "    \"        x = self.bn3(x)\\n\",\n",
    "    \"        x = self.relu(x)\\n\",\n",
    "    \"        x = self.maxpool(x)\\n\",\n",
    "    \"        x = self.dropout(x)\\n\",\n",
    "    \"        x = x.view(x.size(0), -1)\\n\",\n",
    "    \"        x = self.fc1(x)\\n\",\n",
    "    \"        x = self.relu(x)\\n\",\n",
    "    \"        x = self.dropout(x)\\n\",\n",
    "    \"        x = self.fc2(x)\\n\",\n",
    "    \"        return x\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\\n\",\n",
    "    \"    train_losses = []\\n\",\n",
    "    \"    val_losses = []\\n\",\n",
    "    \"    train_accs = []\\n\",\n",
    "    \"    val_accs = []\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for epoch in range(num_epochs):\\n\",\n",
    "    \"        # Training phase\\n\",\n",
    "    \"        model.train()\\n\",\n",
    "    \"        running_loss = 0.0\\n\",\n",
    "    \"        correct = 0\\n\",\n",
    "    \"        total = 0\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        for inputs, labels in train_loader:\\n\",\n",
    "    \"            optimizer.zero_grad()\\n\",\n",
    "    \"            outputs = model(inputs)\\n\",\n",
    "    \"            loss = criterion(outputs, labels)\\n\",\n",
    "    \"            loss.backward()\\n\",\n",
    "    \"            optimizer.step()\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            running_loss += loss.item()\\n\",\n",
    "    \"            _, predicted = torch.max(outputs.data, 1)\\n\",\n",
    "    \"            total += labels.size(0)\\n\",\n",
    "    \"            correct += (predicted == labels).sum().item()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        train_loss = running_loss / len(train_loader)\\n\",\n",
    "    \"        train_acc = 100 * correct / total\\n\",\n",
    "    \"        train_losses.append(train_loss)\\n\",\n",
    "    \"        train_accs.append(train_acc)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Validation phase\\n\",\n",
    "    \"        model.eval()\\n\",\n",
    "    \"        val_loss = 0.0\\n\",\n",
    "    \"        correct = 0\\n\",\n",
    "    \"        total = 0\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        with torch.no_grad():\\n\",\n",
    "    \"            for inputs, labels in val_loader:\\n\",\n",
    "    \"                outputs = model(inputs)\\n\",\n",
    "    \"                loss = criterion(outputs, labels)\\n\",\n",
    "    \"                val_loss += loss.item()\\n\",\n",
    "    \"                _, predicted = torch.max(outputs.data, 1)\\n\",\n",
    "    \"                total += labels.size(0)\\n\",\n",
    "    \"                correct += (predicted == labels).sum().item()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        val_loss = val_loss / len(val_loader)\\n\",\n",
    "    \"        val_acc = 100 * correct / total\\n\",\n",
    "    \"        val_losses.append(val_loss)\\n\",\n",
    "    \"        val_accs.append(val_acc)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        print(f'Epoch {epoch+1}/{num_epochs}:')\\n\",\n",
    "    \"        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\\n\",\n",
    "    \"        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\\n\",\n",
    "    \"        print('-' * 50)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return train_losses, val_losses, train_accs, val_accs\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def evaluate_model(model, test_loader):\\n\",\n",
    "    \"    model.eval()\\n\",\n",
    "    \"    all_preds = []\\n\",\n",
    "    \"    all_labels = []\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    with torch.no_grad():\\n\",\n",
    "    \"        for inputs, labels in test_loader:\\n\",\n",
    "    \"            outputs = model(inputs)\\n\",\n",
    "    \"            _, predicted = torch.max(outputs.data, 1)\\n\",\n",
    "    \"            all_preds.extend(predicted.numpy())\\n\",\n",
    "    \"            all_labels.extend(labels.numpy())\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    accuracy = accuracy_score(all_labels, all_preds)\\n\",\n",
    "    \"    precision = precision_score(all_labels, all_preds, average='weighted')\\n\",\n",
    "    \"    recall = recall_score(all_labels, all_preds, average='weighted')\\n\",\n",
    "    \"    f1 = f1_score(all_labels, all_preds, average='weighted')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return accuracy, precision, recall, f1\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Train and evaluate all three models\\n\",\n",
    "    \"models = {\\n\",\n",
    "    \"    'Simple CNN': SimpleCNN(),\\n\",\n",
    "    \"    'Medium CNN': MediumCNN(),\\n\",\n",
    "    \"    'Complex CNN': ComplexCNN()\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"results = {}\\n\",\n",
    "    \"\\n\",\n",
    "    \"for name, model in models.items():\\n\",\n",
    "    \"    print(f'\\\\nTraining {name}...')\\n\",\n",
    "    \"    criterion = nn.CrossEntropyLoss()\\n\",\n",
    "    \"    optimizer = optim.Adam(model.parameters(), lr=0.001)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    train_losses, val_losses, train_accs, val_accs = train_model(\\n\",\n",
    "    \"        model, train_loader, val_loader, criterion, optimizer, num_epochs=10\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    accuracy, precision, recall, f1 = evaluate_model(model, test_loader)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    results[name] = {\\n\",\n",
    "    \"        'accuracy': accuracy,\\n\",\n",
    "    \"        'precision': precision,\\n\",\n",
    "    \"        'recall': recall,\\n\",\n",
    "    \"        'f1': f1,\\n\",\n",
    "    \"        'train_losses': train_losses,\\n\",\n",
    "    \"        'val_losses': val_losses,\\n\",\n",
    "    \"        'train_accs': train_accs,\\n\",\n",
    "    \"        'val_accs': val_accs\\n\",\n",
    "    \"    }\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Create comparison table\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"\\n\",\n",
    "    \"comparison_data = {\\n\",\n",
    "    \"    'Model': [],\\n\",\n",
    "    \"    'Accuracy': [],\\n\",\n",
    "    \"    'Precision': [],\\n\",\n",
    "    \"    'Recall': [],\\n\",\n",
    "    \"    'F1 Score': []\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"for name, metrics in results.items():\\n\",\n",
    "    \"    comparison_data['Model'].append(name)\\n\",\n",
    "    \"    comparison_data['Accuracy'].append(f'{metrics[\\\"accuracy\\\"]:.4f}')\\n\",\n",
    "    \"    comparison_data['Precision'].append(f'{metrics[\\\"precision\\\"]:.4f}')\\n\",\n",
    "    \"    comparison_data['Recall'].append(f'{metrics[\\\"recall\\\"]:.4f}')\\n\",\n",
    "    \"    comparison_data['F1 Score'].append(f'{metrics[\\\"f1\\\"]:.4f}')\\n\",\n",
    "    \"\\n\",\n",
    "    \"comparison_df = pd.DataFrame(comparison_data)\\n\",\n",
    "    \"print('\\\\nModel Comparison:')\\n\",\n",
    "    \"print(comparison_df.to_string(index=False))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Plot training and validation metrics\\n\",\n",
    "    \"def plot_metrics(results):\\n\",\n",
    "    \"    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\\n\",\n",
    "    \"    fig.suptitle('Model Performance Comparison')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Plot training loss\\n\",\n",
    "    \"    for name, metrics in results.items():\\n\",\n",
    "    \"        axes[0, 0].plot(metrics['train_losses'], label=name)\\n\",\n",
    "    \"    axes[0, 0].set_title('Training Loss')\\n\",\n",
    "    \"    axes[0, 0].set_xlabel('Epoch')\\n\",\n",
    "    \"    axes[0, 0].set_ylabel('Loss')\\n\",\n",
    "    \"    axes[0, 0].legend()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Plot validation loss\\n\",\n",
    "    \"    for name, metrics in results.items():\\n\",\n",
    "    \"        axes[0, 1].plot(metrics['val_losses'], label=name)\\n\",\n",
    "    \"    axes[0, 1].set_title('Validation Loss')\\n\",\n",
    "    \"    axes[0, 1].set_xlabel('Epoch')\\n\",\n",
    "    \"    axes[0, 1].set_ylabel('Loss')\\n\",\n",
    "    \"    axes[0, 1].legend()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Plot training accuracy\\n\",\n",
    "    \"    for name, metrics in results.items():\\n\",\n",
    "    \"        axes[1, 0].plot(metrics['train_accs'], label=name)\\n\",\n",
    "    \"    axes[1, 0].set_title('Training Accuracy')\\n\",\n",
    "    \"    axes[1, 0].set_xlabel('Epoch')\\n\",\n",
    "    \"    axes[1, 0].set_ylabel('Accuracy (%)')\\n\",\n",
    "    \"    axes[1, 0].legend()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Plot validation accuracy\\n\",\n",
    "    \"    for name, metrics in results.items():\\n\",\n",
    "    \"        axes[1, 1].plot(metrics['val_accs'], label=name)\\n\",\n",
    "    \"    axes[1, 1].set_title('Validation Accuracy')\\n\",\n",
    "    \"    axes[1, 1].set_xlabel('Epoch')\\n\",\n",
    "    \"    axes[1, 1].set_ylabel('Accuracy (%)')\\n\",\n",
    "    \"    axes[1, 1].legend()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    plt.tight_layout()\\n\",\n",
    "    \"    plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"plot_metrics(results)\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.8.0\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
